<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="lv wangjing"><meta name="copyright" content="lv wangjing"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>Attention | 小吕的小窝</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"enchantedll.github.io","root":"/","title":"小吕的秘密树洞","version":"1.6.2","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="preconnect" href="https://www.google-analytics.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-1LL0D86CY9"></script><script>if (CONFIG.hostname === location.hostname) {
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1LL0D86CY9');
}</script><script data-ad-client="ca-pub-2245427233262012" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-M9KWR9L');</script><!-- End Google Tag Manager --><meta name="description" content="Seq2Seq模型Seq2Seq模型在机器翻译中的应用~ 数据处理 把大写字母全部转化为小写，去掉句子中的标点符号； Tokenization：把一句话变成很多个单词或者很多个字符；在进行tokenization的时候需要Use 2 difference tokenizers for 2 languages.(使用2个tokenizers的原因是不同的语言，有不同的字母表。)Tokenizatio">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention">
<meta property="og:url" content="http://enchantedll.github.io/2021/08/11/Attention/index.html">
<meta property="og:site_name" content="小吕的小窝">
<meta property="og:description" content="Seq2Seq模型Seq2Seq模型在机器翻译中的应用~ 数据处理 把大写字母全部转化为小写，去掉句子中的标点符号； Tokenization：把一句话变成很多个单词或者很多个字符；在进行tokenization的时候需要Use 2 difference tokenizers for 2 languages.(使用2个tokenizers的原因是不同的语言，有不同的字母表。)Tokenizatio">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Seq1.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Attention1.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Attention2.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Attention3.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Attention4.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Attention5.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Attention6.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Self-Attention1.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Self-Attention2.png">
<meta property="og:image" content="http://enchantedll.github.io/images/Attention/Self-Attention3.png">
<meta property="article:published_time" content="2021-08-11T08:11:42.414Z">
<meta property="article:modified_time" content="2021-08-21T12:45:44.660Z">
<meta property="article:author" content="lv wangjing">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://enchantedll.github.io/images/Attention/Seq1.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="lv wangjing"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="lv wangjing"><span class="site-author-status" title="欢迎来到我的博客">😋</span></a><div class="site-author-name"><a href="/about/">lv wangjing</a></div><span class="site-name">小吕的小窝</span><sub class="site-subtitle"></sub><div class="site-desciption"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">6</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="留言板"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Enchantedll" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1345539017@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Seq2Seq%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">Seq2Seq模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">数据处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-Seq2Seq-Model"><span class="toc-number">1.2.</span> <span class="toc-text">Training Seq2Seq Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference-Using-the-Seq2Seq-Model"><span class="toc-number">1.3.</span> <span class="toc-text">Inference Using the Seq2Seq Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-improve-Seq2Seq-Model"><span class="toc-number">1.4.</span> <span class="toc-text">How to improve Seq2Seq Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention%E6%9C%BA%E5%88%B6"><span class="toc-number">2.</span> <span class="toc-text">Attention机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">Attention的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">2.2.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Attention"><span class="toc-number">3.</span> <span class="toc-text">Self-Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SimpleRNN-Self-Attention"><span class="toc-number">3.1.</span> <span class="toc-text">SimpleRNN + Self-Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-1"><span class="toc-number">3.2.</span> <span class="toc-text">Summary</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://Enchantedll.github.io/2021/08/11/Attention/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="lv wangjing"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="小吕的小窝"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Attention<a class="post-edit-link" href="https://github.com/YunYouJun/yunyoujun.github.io/tree/hexo/source/_posts/Attention.md" target="_blank" title="编辑" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2021-08-11 16:11:42" itemprop="dateCreated datePublished" datetime="2021-08-11T16:11:42+08:00">2021-08-11</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2021-08-21 20:45:44" itemprop="dateModified" datetime="2021-08-21T20:45:44+08:00">2021-08-21</time></div><div class="post-classify"><span class="post-tag"><a class="tag-item" href="/tags/%E7%AC%94%E8%AE%B0/" style="--text-color:dimgray"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><h2 id="Seq2Seq模型"><a href="#Seq2Seq模型" class="headerlink" title="Seq2Seq模型"></a>Seq2Seq模型</h2><p>Seq2Seq模型在机器翻译中的应用~</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><ol>
<li>把大写字母全部转化为小写，去掉句子中的标点符号；</li>
<li>Tokenization：把一句话变成很多个单词或者很多个字符；在进行tokenization的时候需要Use 2 difference tokenizers for 2 languages.(使用2个tokenizers的原因是不同的语言，有不同的字母表。)<br>Tokenization可以是word-level也可以是char-level；</li>
<li>Build 2 different dictionaries.需要向目标语言的字典中，加两个符号一个是“起始符”，一个是“终止符”；</li>
<li>使用字典可以将字符映射成整数，这样一句语料便变成了一个Seq<br><img src="/images/Attention/Seq1.png" alt="Seq" loading="lazy"><br>如上图所示，将整数元素变成One Hot向量做完One Hot Encodering每个单词就用一个向量表示，每句话就用一个矩阵表示。这个矩阵就是RNN的输入。</li>
</ol>
<h3 id="Training-Seq2Seq-Model"><a href="#Training-Seq2Seq-Model" class="headerlink" title="Training Seq2Seq Model"></a>Training Seq2Seq Model</h3><p>Seq2Seq模型由一个Encodering编码器和一个Decodering解码器组成。</p>
<p>Encoder用来从输入RNN的句子中提取特征。Encoder的最后一个状态就包含了输入的句子的信息。<br>Decoder的初始状态是Encoder的最后一个状态。</p>
<ol>
<li>Decoder的第一个输入是一个“起始符”，Decoder会输出一个概率分布，记作向量p；</li>
<li>将“起始符”后面一个字母作为标签y；</li>
<li>用标签y和预测p的CrossEntropy作为损失函数（我们希望预测p更可能的接近标签y，所以损失函数越小越好）；</li>
<li>有了损失函数就可以反向传播计算梯度，从损失函数传到Decoder再从Decoder传到Encoder，然后使用梯度下降来更新Encoder和Decoder的模型参数，让损失函数减小。</li>
<li>然后输入是两个字符“起始符和第一个字符”，Decoder会输出对下一个字符的预测；</li>
<li>以此类推：重复1-5</li>
</ol>
<h3 id="Inference-Using-the-Seq2Seq-Model"><a href="#Inference-Using-the-Seq2Seq-Model" class="headerlink" title="Inference Using the Seq2Seq Model"></a>Inference Using the Seq2Seq Model</h3><ol>
<li>将英语的每一个字符输入Encoder，Encoder会在状态h和c中积累这段信息，Encoder会输出最后的状态，这是从这句话中提取的向量；</li>
<li>Encoder的输出被送给Decoder，被作为Decoder的初始状态；</li>
<li>首先将“起始符”输入Decoder，有了新的输入Decoder就会更新状态向量h和c，并且预测下一个字符；</li>
<li>Decoder输出的预测是每个字符的概率值，我们要根据概率值来选取一个字符，将选取得到的字符记录下来；</li>
<li>现在Decoder的状态已经被更新，将选取记录的字符作为下一次Decoder的输入，让LSTM来预测下一个字符，基于被更新的状态，和新的输入，LSTM会更新状态，并且输出新的概率分布；</li>
<li>不断重复以上更新状态，并且生成新的字符的，并且用新生成的字符作为下一个的输入的过程；</li>
<li>直到预测下一个字符为“终止符”,则停止预测，返回模型记录下的字符串。</li>
</ol>
<h3 id="How-to-improve-Seq2Seq-Model"><a href="#How-to-improve-Seq2Seq-Model" class="headerlink" title="How to improve Seq2Seq Model"></a>How to improve Seq2Seq Model</h3><ol>
<li>BiLSTM instend of LSTM(<strong>Encoder only!!!</strong>)<br>因为假如英语句子很长，LSTM就会遗忘之前的输入，使用BiLSTM可以获得更长的记忆。<br>注：Decoder必须是单向的LSTM，Decoder是一个文本生成器，必须按顺序生成文本。</li>
<li>Word-level instend of char-level<br>char-level的优点：比较方便，不需要embedding层<br>Word-level的优点：输入的序列会更短，序列更短就更不容易遗忘；<br>Word-level的缺点：需要有更大的数据集，有overfitting的问题</li>
<li>多任务学习</li>
<li>Attention机制</li>
</ol>
<h2 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h2><p>Seq2Seq Model有一个缺点：如果输入的句子很长，Encoder最后一个状态，可能会漏掉一些信息，Decoder就无从得知完整的信息<br>解决Seq2Seq遗忘问题最好的办法就是加入Attention机制，有了Attention，Decoder每次更新状态的时候，都会重新看一下Encoder的所有状态，这样就不会遗忘；Attention还会告诉Decoder应该关注Encoder哪个状态。<br>Attention的优点：大幅提高准确率<br>Attention的缺点：计算量巨大</p>
<h3 id="Attention的原理"><a href="#Attention的原理" class="headerlink" title="Attention的原理"></a>Attention的原理</h3><ol>
<li>在Encoder结束工作之后，Decoder和Attention同时开始工作；Decoder的初始状态是S0的Encoder的最后一个状态Hm；</li>
<li>我们需要记录下Encoder的每一个状态H0，H1，H2……Hm,然后用以下公式计算S0与每一个H的相关性,被记为权重weight；(权重weight都是介于0-1之间的实数，所有的weight加起来等于1)<br><img src="/images/Attention/Attention1.png" alt="相关性计算公式" loading="lazy"><br>计算相关性weight的方法有很多：<br>方法一：如下图所示：<br><img src="/images/Attention/Attention2.png" alt="weight具体计算方法1" loading="lazy"><br>注：计算向量v和以上计算得到的向量的内积，向量v和矩阵w都是参数，需要从训练数据中学习。同时通过softmax的作用，使weight都值都介于0-1之间，并且何为1；<br>方法二：如下图所示：<br><img src="/images/Attention/Attention3.png" alt="weight具体计算方法2" loading="lazy"><br>首先是使用两个参数矩阵Wk和Wq(两个参数矩阵通过训练数据训练得到)对两个输入向量做线性变换；得到Ki和Q0两个向量；<br>然后计算Ki和Q0这两个向量的内积；<br>最后将得到的内积进行normilazation；<br>以上计算权重的办法被Transform模型所采用。</li>
<li>每一个权重weight有和他相对应的H，利用权重weight可以对H求加权平均，计算公式如下图所示:计算得到的结果是一个向量，我们把它记为Context vector;(因为C0是一个加权平均，所以保存了完整的输入)<br><img src="/images/Attention/Attention4.png" alt="Context vector的计算方法" loading="lazy"></li>
<li>每一个Context vector都有一个对应的Decoder状态S；如C0对应的的Decoder状态就是S0；</li>
<li>使用以下公式更新状态S为S1；(因为S1的计算和C0有关，所以同样的，S1也知道了完整的输入，从而解决了遗忘问题)<br><img src="/images/Attention/Attention5.png" alt="S的计算方法" loading="lazy"></li>
<li>重复以上过程，计算C1，其中加权平均所要用到的状态是H与S1的相关性<br>关于Attention的时间复杂度：对于Decoder的T个状态，都需要计算S与H的m（m为Encoder的状态数）个相关性，所以一共需要计算m*t个weight，时间复杂度为wt；<br>所以Attention可以避免遗忘，大幅提高准确率，但是同时了增大了时间复杂度。<br>Attention的优势除了能够获得更长的记忆以外；当计算Decoder时，权重weight会告诉Decoder应该关注Encoder的哪一个状态。</li>
</ol>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/images/Attention/Attention6.png" alt="Summary" loading="lazy"></p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>将Attention用在同一个RNN网络上就是Self-Attention;Attention并不局限在Seq2Seq模型上，Attention可以用在所有的RNN上</p>
<h3 id="SimpleRNN-Self-Attention"><a href="#SimpleRNN-Self-Attention" class="headerlink" title="SimpleRNN + Self-Attention"></a>SimpleRNN + Self-Attention</h3><ol>
<li>初始的时候，状态向量H和Context vector都是全0向量；</li>
<li>RNN读入第一个输入X1，需要更新状态H，要把读入的X1的信息压缩到状态H中，计算公式如下图所示：<br><img src="/images/Attention/Self-Attention1.png" alt="SimpleRNN+Self-Attention中H的计算公式" loading="lazy"></li>
<li>C1是已有状态H的加权平均(因为H0是全0向量，所以C1=H1)</li>
<li>读入X2，用C1和X2做连结得到的向量，计算H2；</li>
<li>接着计算新的Context vectorC2，想要计算C2首先要先计算所有H(包括H2自身)与最新计算得到的H2的相关性，得到weight，C2的计算还是计算加权平均<br><img src="/images/Attention/Self-Attention2.png" alt="weight的计算方法" loading="lazy"></li>
<li>不断重复以上过程，读入C计算H和C；</li>
</ol>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/images/Attention/Self-Attention3.png" alt="Summary" loading="lazy"></p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>lv wangjing</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://enchantedll.github.io/2021/08/11/Attention/" title="Attention">http://enchantedll.github.io/2021/08/11/Attention/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/08/09/BiLSTMCRF/" rel="next" title="BiLSTMCRF"><span class="post-nav-text">BiLSTMCRF</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>要不要和我说些什么？</span><br></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> lv wangjing</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v5.4.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.2</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div></body></html>