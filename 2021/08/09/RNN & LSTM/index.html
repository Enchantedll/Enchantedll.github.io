<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="lv wangjing"><meta name="copyright" content="lv wangjing"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>RNN &amp; LSTM | 小吕的小窝</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"enchantedll.github.io","root":"/","title":"小吕的秘密树洞","version":"1.6.2","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="preconnect" href="https://www.google-analytics.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-1LL0D86CY9"></script><script>if (CONFIG.hostname === location.hostname) {
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1LL0D86CY9');
}</script><script data-ad-client="ca-pub-2245427233262012" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-M9KWR9L');</script><!-- End Google Tag Manager --><meta name="description" content="算是了解BiLSTMCRF的前传，虽然用学长的话来说：“如果从RNN开始讲，就像是讲人类历史，你从盘古开天辟地开始讲。”（咳咳，srds了解了的东西就先记录一下吧）。 RNNRNN（即循环神经网络）：对于文本问题，因为其输入和输出的长度都不确定，所以一般会使用RNN来解决问题。RNN工作的主要原理：通过状态向量h来积累阅读过的信息。RNN进行工作的一般步骤可以分为以下几步：  将输入的词word通">
<meta property="og:type" content="article">
<meta property="og:title" content="RNN &amp; LSTM">
<meta property="og:url" content="http://enchantedll.github.io/2021/08/09/RNN%20&%20LSTM/index.html">
<meta property="og:site_name" content="小吕的小窝">
<meta property="og:description" content="算是了解BiLSTMCRF的前传，虽然用学长的话来说：“如果从RNN开始讲，就像是讲人类历史，你从盘古开天辟地开始讲。”（咳咳，srds了解了的东西就先记录一下吧）。 RNNRNN（即循环神经网络）：对于文本问题，因为其输入和输出的长度都不确定，所以一般会使用RNN来解决问题。RNN工作的主要原理：通过状态向量h来积累阅读过的信息。RNN进行工作的一般步骤可以分为以下几步：  将输入的词word通">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/RNN1.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/RNN2.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/tanh.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/RNN3.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/LSTM1.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/LSTM2.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/ForgetGate.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/InputGate.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/NewValue.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/OutputGate.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/Ct.png">
<meta property="og:image" content="http://enchantedll.github.io/images/RNNLSTM/Ht.png">
<meta property="article:published_time" content="2021-08-09T02:25:42.895Z">
<meta property="article:modified_time" content="2021-08-21T12:43:30.387Z">
<meta property="article:author" content="lv wangjing">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://enchantedll.github.io/images/RNNLSTM/RNN1.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="lv wangjing"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="lv wangjing"><span class="site-author-status" title="欢迎来到我的博客">😋</span></a><div class="site-author-name"><a href="/about/">lv wangjing</a></div><span class="site-name">小吕的小窝</span><sub class="site-subtitle"></sub><div class="site-desciption"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">6</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">1</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="留言板"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Enchantedll" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1345539017@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN"><span class="toc-number">1.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Simple-RNN%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="toc-number">1.1.</span> <span class="toc-text">Simple RNN计算公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Simple-RNN%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">Simple RNN的特点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM"><span class="toc-number">2.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Forget-Gate"><span class="toc-number">2.1.</span> <span class="toc-text">Forget Gate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Input-Gate"><span class="toc-number">2.2.</span> <span class="toc-text">Input Gate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#New-Value"><span class="toc-number">2.3.</span> <span class="toc-text">New Value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Gate"><span class="toc-number">2.4.</span> <span class="toc-text">Output Gate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ct"><span class="toc-number">2.5.</span> <span class="toc-text">Ct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ht"><span class="toc-number">2.6.</span> <span class="toc-text">Ht</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://Enchantedll.github.io/2021/08/09/RNN%20&amp;%20LSTM/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="lv wangjing"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="小吕的小窝"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">RNN &amp; LSTM<a class="post-edit-link" href="https://github.com/YunYouJun/yunyoujun.github.io/tree/hexo/source/_posts/RNN &amp; LSTM.md" target="_blank" title="编辑" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2021-08-09 10:25:42" itemprop="dateCreated datePublished" datetime="2021-08-09T10:25:42+08:00">2021-08-09</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2021-08-21 20:43:30" itemprop="dateModified" datetime="2021-08-21T20:43:30+08:00">2021-08-21</time></div><div class="post-classify"><span class="post-tag"><a class="tag-item" href="/tags/%E7%AC%94%E8%AE%B0/" style="--text-color:dimgray"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">笔记</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><p>算是了解BiLSTMCRF的前传，虽然用学长的话来说：“如果从RNN开始讲，就像是讲人类历史，你从盘古开天辟地开始讲。”（咳咳，srds了解了的东西就先记录一下吧）。</p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>RNN（即循环神经网络）：对于文本问题，因为其输入和输出的长度都不确定，所以一般会使用RNN来解决问题。<br>RNN工作的主要原理：通过状态向量h来积累阅读过的信息。<br><img src="/images/RNNLSTM/RNN1.png" alt="RNN的原理图" loading="lazy"><br>RNN进行工作的一般步骤可以分为以下几步：</p>
<ol>
<li>将输入的词word通过word embedding映射成词向量x；</li>
<li>将词向量输入RNN，RNN更新状态向量h，将新的输入积累到新的状态h中。<br>注：h0中包含的是第一个词的信息；h1中包含的是前两个词的信息……以此类推ht中包含的是前(t+1)个词的信息。<br>更新状态向量h时，需要用到参数矩阵A（这里需要注意的是，参数矩阵A在整个RNN中都是不变的，唯一的，同时参数矩阵A随机初始化，利用训练数据来学习得到A）</li>
</ol>
<h3 id="Simple-RNN计算公式"><a href="#Simple-RNN计算公式" class="headerlink" title="Simple RNN计算公式"></a>Simple RNN计算公式</h3><p><img src="/images/RNNLSTM/RNN2.png" alt="RNN的计算公式" loading="lazy"><br>如上图所示，RNN的计算方法可分为以下几个步骤：</p>
<ol>
<li>将新输入的词向量Xt与上一个状态向量H(t-1)做contatenation(连结)，得到一个更高维度的向量；</li>
<li>将参数矩阵A与1中得到的高维向量做乘积得到一个与1中维度一样的向量；</li>
<li>将激活函数tanh(双曲正切函数)作用于2中所得向量的每一个元素上，得到RNN的输出Ht。<br><img src="/images/RNNLSTM/tanh.png" alt="tanh的函数图像" loading="lazy"><br>激活函数的函数图像如上图所示，是一个双曲正切函数；<br>它的主要作用：将Ht的每一个元素都压缩到[-1,1]这个合适的区间。<br><img src="/images/RNNLSTM/RNN3.png" alt="假如我们去掉tanh会怎么样呢" loading="lazy"><br>如上图所示，如果我们去掉tanh：<br>若参数矩阵A最大的特征值略小于1，为0.9，则A^100会非常接近0，h100的元素也会变成0；<br>若参数矩阵A最大的特征值略大于1，为1.2，则A^100会变得非常大，h100的元素会爆炸。</li>
</ol>
<p>通过计算公式可以看出参数矩阵A的维度为：shape(h) * [shape(h) + shape(x)]</p>
<h3 id="Simple-RNN的特点"><a href="#Simple-RNN的特点" class="headerlink" title="Simple RNN的特点"></a>Simple RNN的特点</h3><p>Simple RNN is good at short-term dependence,but is bad at long-term dependence.</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM(Long short-term memory)是对RNN模型的一种改进，可以避免记录消失的问题，可以获得更长的记忆。<br>LSTM中最重要的就是传送带Ct的设计，过去的信息通过传送带直接传送到下一时刻，由此通过Ct来避免记录消失的问题。</p>
<p><img src="/images/RNNLSTM/LSTM1.png" alt="RNN &amp; LSTM的原理图" loading="lazy"><br>LSTM的原理如上图所示</p>
<h3 id="Forget-Gate"><a href="#Forget-Gate" class="headerlink" title="Forget Gate"></a>Forget Gate</h3><p>Forget Gate的作用：遗忘门的输出ft与Ct做Elementwise multiplication,有选择性的让传送带Ct中的内容通过；</p>
<ul>
<li>若ft中某一元素的值为1，则ft对应的Ct中的元素输出为原来的Ct值，全部通过；</li>
<li>若ft中某一元素的值为0，则ft对应的Ct中的元素输出为0，Ct中对应的元素不能通过。<br><img src="/images/RNNLSTM/LSTM2.png" alt="Forget Gate" loading="lazy"><br><img src="/images/RNNLSTM/ForgetGate.png" alt="Forget Gate的计算方法" loading="lazy"><br>上图所示为Ft的具体计算方法，其中Wf为参数矩阵，需要通过反向传播从训练数据中学习得到；sigmoid function的作用是作用到计算结果向量的每一个元素，把每一个元素都压缩到[0,1]之间。</li>
</ul>
<h3 id="Input-Gate"><a href="#Input-Gate" class="headerlink" title="Input Gate"></a>Input Gate</h3><p><img src="/images/RNNLSTM/InputGate.png" alt="Input Gate的计算方法" loading="lazy"><br>It的计算方法与Ft的计算方法基本类似，只是参数矩阵不同，为Wi（通过训练得到）。</p>
<h3 id="New-Value"><a href="#New-Value" class="headerlink" title="New Value"></a>New Value</h3><p><img src="/images/RNNLSTM/NewValue.png" alt="New Value的计算方法" loading="lazy"><br>New Value的计算方法类似于Ft、It，只不过参数矩阵为Wc，激活函数为tanh(双曲正切函数)。</p>
<h3 id="Output-Gate"><a href="#Output-Gate" class="headerlink" title="Output Gate"></a>Output Gate</h3><p><img src="/images/RNNLSTM/OutputGate.png" alt="Output Gate的计算方法" loading="lazy"><br>Ot的计算方法与Ft、It基本类似，只是它有自己的参数矩阵Wo。</p>
<h3 id="Ct"><a href="#Ct" class="headerlink" title="Ct"></a>Ct</h3><p><del>(唉，已经晚上10点了，我好困。😪)</del><br><img src="/images/RNNLSTM/Ct.png" alt="Ct的计算公式" loading="lazy"><br>Ct的计算公式如上所示，式子的前半部分，选择性的遗忘C(t-1)中的一些内容，式子的后半部分给传送带上加入一些新的东西。</p>
<h3 id="Ht"><a href="#Ht" class="headerlink" title="Ht"></a>Ht</h3><p><img src="/images/RNNLSTM/Ht.png" alt="Ht的计算公式" loading="lazy"><br>计算得到Ht之后会有两份copy，一份是LSTM的输出，一份传送到下一个LSTM。</p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>lv wangjing</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://enchantedll.github.io/2021/08/09/RNN%20&amp;%20LSTM/" title="RNN &amp; LSTM">http://enchantedll.github.io/2021/08/09/RNN%20&%20LSTM/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2021/08/09/BiLSTMCRF/" rel="prev" title="BiLSTMCRF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">BiLSTMCRF</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/08/08/%E4%BA%8B%E4%BB%B6%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%8E%B0%E6%9C%89%E7%9A%84%E6%96%B9%E6%B3%95/" rel="next" title="事件因果关系抽取现有的方法"><span class="post-nav-text">事件因果关系抽取现有的方法</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>要不要和我说些什么？</span><br></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> lv wangjing</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v5.4.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.2</span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div></body></html>